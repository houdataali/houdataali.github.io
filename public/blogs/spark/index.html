<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>From Concept to Execution: Understanding Apache Spark&#39;s Evolution | Houda TAALI</title>
<meta name="keywords" content="">
<meta name="description" content="Apache Spark has become a powerhouse in the world of big data processing. It&rsquo;s a tool that has revolutionized the way we handle massive datasets, enabling faster and more efficient data processing and analysis. But where did Spark come from, and how did it evolve into the robust framework we know today? In this blog post, we&rsquo;ll take a journey through the history of Apache Spark, exploring its origins and its remarkable evolution from concept to execution.">
<meta name="author" content="">
<link rel="canonical" href="https://houdataali.github.io/blogs/spark/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://houdataali.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://houdataali.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://houdataali.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://houdataali.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://houdataali.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="From Concept to Execution: Understanding Apache Spark&#39;s Evolution" />
<meta property="og:description" content="Apache Spark has become a powerhouse in the world of big data processing. It&rsquo;s a tool that has revolutionized the way we handle massive datasets, enabling faster and more efficient data processing and analysis. But where did Spark come from, and how did it evolve into the robust framework we know today? In this blog post, we&rsquo;ll take a journey through the history of Apache Spark, exploring its origins and its remarkable evolution from concept to execution." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://houdataali.github.io/blogs/spark/" /><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2023-09-24T16:21:49+01:00" />
<meta property="article:modified_time" content="2023-09-24T16:21:49+01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="From Concept to Execution: Understanding Apache Spark&#39;s Evolution"/>
<meta name="twitter:description" content="Apache Spark has become a powerhouse in the world of big data processing. It&rsquo;s a tool that has revolutionized the way we handle massive datasets, enabling faster and more efficient data processing and analysis. But where did Spark come from, and how did it evolve into the robust framework we know today? In this blog post, we&rsquo;ll take a journey through the history of Apache Spark, exploring its origins and its remarkable evolution from concept to execution."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Blogs",
      "item": "https://houdataali.github.io/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "From Concept to Execution: Understanding Apache Spark's Evolution",
      "item": "https://houdataali.github.io/blogs/spark/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "From Concept to Execution: Understanding Apache Spark's Evolution",
  "name": "From Concept to Execution: Understanding Apache Spark\u0027s Evolution",
  "description": "Apache Spark has become a powerhouse in the world of big data processing. It\u0026rsquo;s a tool that has revolutionized the way we handle massive datasets, enabling faster and more efficient data processing and analysis. But where did Spark come from, and how did it evolve into the robust framework we know today? In this blog post, we\u0026rsquo;ll take a journey through the history of Apache Spark, exploring its origins and its remarkable evolution from concept to execution.",
  "keywords": [
    
  ],
  "articleBody": " Apache Spark has become a powerhouse in the world of big data processing. It‚Äôs a tool that has revolutionized the way we handle massive datasets, enabling faster and more efficient data processing and analysis. But where did Spark come from, and how did it evolve into the robust framework we know today? In this blog post, we‚Äôll take a journey through the history of Apache Spark, exploring its origins and its remarkable evolution from concept to execution.\nTable of Contents The Birth of Apache Spark Unleashing Spark‚Äôs Superpowers Spark Execution Resource Management Spark Job Components Spark Job Submission Workflow Conclusion The Birth of Apache Spark üåü Apache Spark‚Äôs journey began in 2009 at UC Berkeley with the advent of Mesos. At that time, the Apache Hadoop ecosystem lacked the YARN resource manager, which led to the inception of a remarkable class project AMPLab. Their audacious goal: to build a cluster management framework.\nThe Emergence of Mesos üöÄ In the year 2009, the emergence of Mesos marked a pivotal moment in the realm of distributed systems. It was conceived to orchestrate the execution and management of distributed applications, offering a promising alternative to traditional approaches.\nSpark: A Testing tool for Mesos üß† It was within this innovative environment that Apache Spark was born, Apache Spark conceived initially as a tool to test the capabilities of Mesos. During this period, the dominant paradigm revolved around MapReduce, a model heavily reliant on disk storage. MapReduce‚Äôs limitations in terms of performance were evident.\nBut Spark was different. It embraced a memory-centric framework, where the majority of operations occurred within the realm of memory. This strategic shift was a deliberate response to the constraints that had long plagued the MapReduce model.\nThe Remarkable Reawakening üí° However, after its initial inception as a Mesos testing tool, Spark briefly faded into the background, perceived merely as a subproject with a specific purpose‚Äîtesting Mesos üòî.\nBut in 2011, something remarkable happened. The world took notice as it became evident that Spark had achieved a significant breakthrough in performance. It outpaced MapReduce by a wide margin, thanks to its unique in-memory execution model. üöÄ\nApache Spark Takes Flight üéâ In 2012, Apache Spark officially ascended to the status of an Apache project, solidifying its position as a revolutionary framework in the world of big data processing.\nUnleashing Spark‚Äôs Superpowers üöÄ Apache Spark, like a superhero in the world of big data, boasts several incredible strengths that set it apart:\nA Unified Stack üß© Spark‚Äôs unified stack brings together a multitude of data processing and analytics tools under one roof. Whether it‚Äôs batch processing, real-time stream processing, machine learning, or graph processing, Spark‚Äôs got it covered. This seamless integration of various data processing tasks makes Spark a true powerhouse.\nMultilingual API Support üåê Spark speaks the language of developers! It offers APIs not just in its native Scala but also in Python, Java, and other languages. This multilingual support ensures that developers can work with Spark using the language they‚Äôre most comfortable with, making it an inclusive and versatile choice.\nMemory-Based Speed Spark‚Äôs brilliance doesn‚Äôt stop there. Unlike traditional models that rely heavily on disk storage, Spark operates predominantly in-memory. This memory-centric approach results in lightning-fast data processing and analysis, transforming the landscape of big data.\nSpark Execution Resources Management üíº On a Single Machine:\nIn a single-machine environment, we have computing resources (CPU, RAM, GPU‚Ä¶) along with an operating system (OS) and multiple applications, as mentioned in the figure below. The (OS) is responsible for resource management, which includes tasks such as resource scheduling and determining the allocation of resources (CPU, Memory, etc.) to individual applications. The allocation is based on available computing resources and the concurrent applications running on your computer. In a Cluster:\nIn a cluster, multiple working nodes are interconnected to form a cohesive computing environment. These nodes collaborate to perform distributed data processing tasks efficiently. In a cluster, there are dedicated applications for resource management that orchestrate the allocation of resources, These applications include Cluster manager Node manager Spark job components To run your spark application in a cluster, you have to ask the cluster manager for the needed resources allocation. üßô‚Äç‚ôÇÔ∏è It‚Äôs the application master‚Äôs duty to ensure you get the resources you need. This ‚Äúmiddleman‚Äù serves as the entry point to the cluster, making it all happen.\nRunning a Spark application in a cluster involves the following key components:\nSpark application:\nEach application running on the cluster has its own Application Master. The Application Master is responsible for negotiating resources with the Resource Manager, managing the application‚Äôs lifecycle, and ensuring that the application gets the resources it needs.\nSpark driver process:\nBreaks down the Spark app logic into smaller tasks. Distributes and schedules the work among Spark executors. Tracks the status of all the Spark executors and monitor their progress Note: If an executor crashes, it‚Äôs the driver process that requests the cluster manager for additional resources üöí.\nSpark executor processes:\nExecute the tasks assigned by the Spark driver process. Report their status back to the Spark driver Spark Job Submission Workflow The above image illustrates the ‚öôÔ∏è smooth process of Spark job submission:\nYou submit your Spark application to Master Node that interacts with the cluster manager and requests it to take charge of it and manage it. Note: Master node refers to the node where you submit your Spark application.\nThe Spark driver process needs to be started, so the cluster manager will request the node manager to start the driver process with the configuration we have specified during app submission. If node manager has enough resources, then the Spark driver process will be created üöÄ .\nThe Spark driver process will request the cluster manager for additional resources to allocate resources for executor processes.\nThe cluster manager will request the node managers to create the executor processes, the node manager will create the executor process if it has enough resources.\nNote: The node managers that have created executors send informations about their locations‚Ä¶ to the cluster manager üì°.\nThe cluster manager inform back the driver process about the location of executor processes üì£.\nNow, the Spark application is running and the Driver process starts to distribute tasks among teh executors.\nNote: All the Spark application logic will be sent to executors in the form of tasks to do teh computation in parallel, so the spark driver process is the heart of spark app.\nThis orchestrated flow ensures that your Spark job is executed efficiently within the cluster.\nConclusion Exploring Apache Spark‚Äôs history has revealed its innovative origins and the ‚Äòwhy‚Äô behind its existence üå±. Understanding its origins provides a deeper appreciation for the impact it has on the world of data processing. In this article, we‚Äôve tried to cover essential aspects of Apache Spark, from its history to its submission workflow. I hope you‚Äôve found this article both informative and insightful, if you have any questions or comments, please reach me out. Thanks for reading!\nApache Spark Mesos Data ",
  "wordCount" : "1169",
  "inLanguage": "en",
  "datePublished": "2023-09-24T16:21:49+01:00",
  "dateModified": "2023-09-24T16:21:49+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://houdataali.github.io/blogs/spark/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Houda TAALI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://houdataali.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://houdataali.github.io" accesskey="h" title="Houda TAALI (Alt + H)">Houda TAALI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://houdataali.github.io/about-me/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="https://houdataali.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://houdataali.github.io/blogs/" title="Blogs">
                    <span>Blogs</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      From Concept to Execution: Understanding Apache Spark&#39;s Evolution
    </h1>
    <div class="post-meta"><span title='2023-09-24 16:21:49 +0100 +01'>September 24, 2023</span>

</div>
  </header> 
  <div class="post-content"><div style="text-align: justify;">
<p><em>Apache Spark has become a powerhouse in the world of big data processing. It&rsquo;s a tool that has revolutionized the way we handle massive datasets, enabling faster and more efficient data processing and analysis. But where did Spark come from, and how did it evolve into the robust framework we know today? In this blog post, we&rsquo;ll take a journey through the history of Apache Spark, exploring its origins and its remarkable evolution from concept to execution.</em></p>
<h2 id="table-of-contents">Table of Contents<a hidden class="anchor" aria-hidden="true" href="#table-of-contents">#</a></h2>
<ul>
<li><a href="#the-birth-of-apache-spark">The Birth of Apache Spark</a></li>
<li><a href="#unleashing-sparks-superpowers">Unleashing Spark&rsquo;s Superpowers</a></li>
<li><a href="#spark-execution">Spark Execution</a>
<ul>
<li><a href="#resource-management">Resource Management</a></li>
<li><a href="#spark-job-components">Spark Job Components</a></li>
<li><a href="#spark-job-submission-workflow">Spark Job Submission Workflow</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="the-birth-of-apache-spark">The Birth of Apache Spark üåü<a hidden class="anchor" aria-hidden="true" href="#the-birth-of-apache-spark">#</a></h2>
<p>Apache Spark&rsquo;s journey began in 2009 at UC Berkeley with the advent of Mesos. At that time, the Apache Hadoop ecosystem lacked the YARN resource manager, which led to the inception of a remarkable class project AMPLab. Their audacious goal: to build a cluster management framework.</p>
<h3 id="the-emergence-of-mesos-">The Emergence of Mesos üöÄ<a hidden class="anchor" aria-hidden="true" href="#the-emergence-of-mesos-">#</a></h3>
<p>In the year 2009, the emergence of Mesos marked a pivotal moment in the realm of distributed systems. It was conceived to orchestrate the execution and management of distributed applications, offering a promising alternative to traditional approaches.</p>
<h3 id="spark-a-testing-tool-for-mesos-">Spark: A Testing tool for Mesos üß†<a hidden class="anchor" aria-hidden="true" href="#spark-a-testing-tool-for-mesos-">#</a></h3>
<p>It was within this innovative environment that Apache Spark was born, Apache Spark conceived initially as a tool to test the capabilities of Mesos. During this period, the dominant paradigm revolved around MapReduce, a model heavily reliant on disk storage. MapReduce&rsquo;s limitations in terms of performance were evident.</p>
<p>But Spark was different. It embraced a memory-centric framework, where the majority of operations occurred within the realm of memory. This strategic shift was a deliberate response to the constraints that had long plagued the MapReduce model.</p>
<h3 id="the-remarkable-reawakening-">The Remarkable Reawakening üí°<a hidden class="anchor" aria-hidden="true" href="#the-remarkable-reawakening-">#</a></h3>
<p>However, after its initial inception as a Mesos testing tool, Spark briefly faded into the background, perceived merely as a subproject with a specific purpose‚Äîtesting Mesos üòî.</p>
<p>But in 2011, something remarkable happened. The world took notice as it became evident that Spark had achieved a significant breakthrough in performance. It outpaced MapReduce by a wide margin, thanks to its unique in-memory execution model. üöÄ</p>
<h3 id="apache-spark-takes-flight-">Apache Spark Takes Flight üéâ<a hidden class="anchor" aria-hidden="true" href="#apache-spark-takes-flight-">#</a></h3>
<p>In 2012, Apache Spark officially ascended to the status of an Apache project, solidifying its position as a revolutionary framework in the world of big data processing.</p>
<h2 id="unleashing-sparks-superpowers">Unleashing Spark&rsquo;s Superpowers üöÄ<a hidden class="anchor" aria-hidden="true" href="#unleashing-sparks-superpowers">#</a></h2>
<p>Apache Spark, like a superhero in the world of big data, boasts several incredible strengths that set it apart:</p>
<h3 id="a-unified-stack-">A Unified Stack üß©<a hidden class="anchor" aria-hidden="true" href="#a-unified-stack-">#</a></h3>
<p><img loading="lazy" src="https://www.oreilly.com/api/v2/epubs/9781492050032/files/assets/lesp_0103.png" alt="Unified Stack"  />
</p>
<!-- <center><i <center><i style="color: gray;">Apache Spark's Unified Stack</i></center>  -->
<p>Spark&rsquo;s unified stack brings together a multitude of data processing and analytics tools under one roof. Whether it&rsquo;s batch processing, real-time stream processing, machine learning, or graph processing, Spark&rsquo;s got it covered. This seamless integration of various data processing tasks makes Spark a true powerhouse.</p>
<h3 id="multilingual-api-support-">Multilingual API Support üåê<a hidden class="anchor" aria-hidden="true" href="#multilingual-api-support-">#</a></h3>
<p>Spark speaks the language of developers! It offers APIs not just in its native Scala but also in Python, Java, and other languages. This multilingual support ensures that developers can work with Spark using the language they&rsquo;re most comfortable with, making it an inclusive and versatile choice.</p>
<h3 id="memory-based-speed">Memory-Based Speed<a hidden class="anchor" aria-hidden="true" href="#memory-based-speed">#</a></h3>
<p>Spark&rsquo;s brilliance doesn&rsquo;t stop there. Unlike traditional models that rely heavily on disk storage, Spark operates predominantly in-memory. This memory-centric approach results in lightning-fast data processing and analysis, transforming the landscape of big data.</p>
<h2 id="spark-execution">Spark Execution<a hidden class="anchor" aria-hidden="true" href="#spark-execution">#</a></h2>
<h3 id="resource-management">Resources Management  üíº<a hidden class="anchor" aria-hidden="true" href="#resource-management">#</a></h3>
<ol>
<li>
<p><strong>On a Single Machine:</strong></p>
<ul>
<li>In a single-machine environment, we have computing resources (CPU, RAM, GPU&hellip;) along with an operating system (OS) and multiple applications, as mentioned in the figure below.</li>
<li>The (OS) is responsible for resource management, which includes tasks such as resource scheduling and determining the allocation of resources (CPU, Memory, etc.) to individual applications. The allocation is based on available computing resources and the concurrent applications running on your computer.</li>
</ul>
<center><img src="/single_machine.png" width="300" height="400"></center>    
</li>
<li>
<p><strong>In a Cluster:</strong></p>
<ul>
<li>In a cluster, multiple working nodes are interconnected to form a cohesive computing environment. These nodes collaborate to perform distributed data processing tasks efficiently.
<img loading="lazy" src="/cluster_setup.png" alt="Cluster Setup"  />
</li>
<li>In a cluster, there are dedicated applications for resource management that orchestrate the allocation of resources, These applications include
<ul>
<li>Cluster manager</li>
<li>Node manager</li>
</ul>
<center><img src="/node_cluster_manager.png" width="900" height="200"></center> 
</li>
</ul>
</li>
</ol>
<h3 id="spark-job-components">Spark job components<a hidden class="anchor" aria-hidden="true" href="#spark-job-components">#</a></h3>
<p>To run your spark application in a cluster, you have to ask the cluster manager for the needed resources allocation. üßô‚Äç‚ôÇÔ∏è It&rsquo;s the <strong>application master</strong>&rsquo;s duty to ensure you get the resources you need. This &ldquo;middleman&rdquo; serves as the entry point to the cluster, making it all happen.</p>
<p>Running a Spark application in a cluster involves the following key components:</p>
<ol>
<li>
<p><strong>Spark application:</strong></p>
<p>Each application running on the cluster has its own Application Master. The Application Master is responsible for negotiating resources with the Resource Manager, managing the application&rsquo;s lifecycle, and ensuring that the application gets the resources it needs.</p>
</li>
<li>
<p><strong>Spark driver process:</strong></p>
<ul>
<li>Breaks down the Spark app logic into smaller tasks.</li>
<li>Distributes and schedules the work among Spark executors.</li>
<li>Tracks the status of all the Spark executors and monitor their progress</li>
</ul>
<blockquote>
<p><strong>Note:</strong> If an executor crashes, it&rsquo;s the driver process that requests the cluster manager for additional resources üöí.</p>
</blockquote>
</li>
<li>
<p><strong>Spark executor processes:</strong></p>
<ul>
<li>Execute the tasks assigned by the Spark driver process.</li>
<li>Report their status back to the Spark driver</li>
</ul>
</li>
</ol>
<h3 id="spark-job-submission-workflow">Spark Job Submission Workflow<a hidden class="anchor" aria-hidden="true" href="#spark-job-submission-workflow">#</a></h3>
<p><img loading="lazy" src="/spark_job_workflow.png" alt="Cluster Setup"  />

The above image illustrates the ‚öôÔ∏è smooth process of Spark job submission:</p>
<ol>
<li>You submit your Spark application to Master Node that interacts with the cluster manager and requests it to take charge of it and manage it.</li>
</ol>
<blockquote>
<p><strong>Note:</strong> Master node refers to the node where you submit your Spark application.</p>
</blockquote>
<ol start="2">
<li>The Spark driver process needs to be started, so the cluster manager will request the node manager to start the driver process with the configuration we have specified during app submission.</li>
</ol>
<blockquote>
<p>If node manager has enough resources, then the Spark driver process will be created üöÄ .</p>
</blockquote>
<ol start="3">
<li>
<p>The Spark driver process will request the cluster manager for additional resources to allocate resources for executor processes.</p>
</li>
<li>
<p>The cluster manager will request the node managers to create the executor processes, the node manager will create the executor process if it has enough resources.</p>
</li>
</ol>
<blockquote>
<p><strong>Note:</strong> The node managers that have created executors send informations about their locations&hellip; to the cluster manager üì°.</p>
</blockquote>
<ol start="5">
<li>
<p>The cluster manager inform back the driver process about the location of executor processes üì£.</p>
</li>
<li>
<p>Now, the Spark application is running and the Driver process starts to distribute tasks among teh executors.</p>
</li>
</ol>
<blockquote>
<p><strong>Note:</strong> All the Spark application logic will be sent to executors in the form of tasks to do teh computation in parallel, so the spark driver process is the heart  of spark app.</p>
</blockquote>
<p>This orchestrated flow ensures that your Spark job is executed efficiently within the cluster.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Exploring Apache Spark&rsquo;s history has revealed its innovative origins and the &lsquo;why&rsquo; behind its existence üå±.  Understanding its origins provides a deeper appreciation for the impact it has on the world of data processing. In this article, we&rsquo;ve tried to cover essential aspects of Apache Spark, from its history to its submission workflow. I hope you&rsquo;ve found this article both informative and insightful, if you have any questions or comments, please reach me out. Thanks for reading!<br><br><br></p>
<div style="display: inline-block; background-color: #f0f0f0; border: 1px solid #ccc; padding: 4px 8px; border-radius: 4px;">
  <span style="font-weight: bold;">Apache </span>
</div>
<div style="display: inline-block; background-color: #f0f0f0; border: 1px solid #ccc; padding: 4px 8px; border-radius: 4px;">
  <span style="font-weight: bold;">Spark</span>
</div>
<div style="display: inline-block; background-color: #f0f0f0; border: 1px solid #ccc; padding: 4px 8px; border-radius: 4px;">
  <span style="font-weight: bold;">Mesos</span>
</div>
<div style="display: inline-block; background-color: #f0f0f0; border: 1px solid #ccc; padding: 4px 8px; border-radius: 4px;">
  <span style="font-weight: bold;">Data</span>
</div>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://houdataali.github.io">Houda TAALI</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
